import aiohttp
import asyncio
from utils import extract_product_urls

async def fetch(domain, session):
    """
    Asynchronously fetch HTML content.
    """
    try:
        async with session.get(domain) as response:
            return await response.text()
    except Exception as e:
        print(f"Error fetching {domain}: {e}")
        return None

async def process_domain(domain):
    """
    Process a single domain to extract product URLs.
    """
    async with aiohttp.ClientSession() as session:
        html = await fetch(domain, session)
        if html:
            return extract_product_urls(html, domain)

async def crawl(domains):
    """
    Crawl multiple domains asynchronously.
    """
    tasks = [process_domain(domain) for domain in domains]
    results = await asyncio.gather(*tasks)
    return {domain: result for domain, result in zip(domains, results) if result}
